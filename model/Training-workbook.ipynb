{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a329471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "from model import LofiModel\n",
    "from config import *\n",
    "from train import train\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc35866",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43969ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded weights from ./lofi_model_epoch_200.pth\n",
      "LofiModel(\n",
      "  (encoder): Encoder(\n",
      "    (input_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (lstm): LSTM(128, 512, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    (fc_mu): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (fc_logvar): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (decoder): HierarchicalDecoder(\n",
      "    (z_to_conductor_initial): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (conductor): LSTM(1, 512, num_layers=2, batch_first=True)\n",
      "    (conductor_to_decoder_initial): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (decoder_rnn): LSTM(1, 512, num_layers=2, batch_first=True)\n",
      "    (output_projection): Linear(in_features=512, out_features=384, bias=True)\n",
      "  )\n",
      ")\n",
      "Model has 18.61 M parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LofiModel()\n",
    "model.load_weights(r\"./lofi_model_epoch_200.pth\")\n",
    "print(model)\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M parameters.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0bdafc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cae3b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading dataset with sliding window...\n",
      "Found 4084 MIDI files.\n",
      "Processing MIDI files, normalizing tempo, and extracting segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 4084/4084 [00:53<00:00, 75.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 137752 segments from 4084 files.\n",
      "Dataset split into: Train=117089, Validation=20663\n",
      "Finished preparing dataset.\n",
      "\n",
      "Calculating class weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing dataset for weights: 100%|██████████| 3660/3660 [00:02<00:00, 1336.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated class weights: [0.0011955752270296216, 0.7773081064224243, 0.2214963436126709]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ziemmi13/lofi-vae/b5d57c920c8144d7be7e40246a10b4e9\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "----- Starting training -----\n",
      "-----------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/400 [Training]: 100%|██████████| 3660/3660 [01:44<00:00, 35.14it/s, Loss=0.0131, Recon=0.0131, KL=2.3880, beta=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: Avg Train Loss: 0.2356, Avg Val Loss: 0.2323\n",
      "\n",
      "Generating visualization for epoch 1:\n",
      "Converting tensor to MIDI file at reconstructed.mid...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel free bits\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/last_projekt_zespolowy/Lofi-MIDI-VAE-generator/model/train.py:150\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, early_stopping, experiment_name, verbose)\u001b[39m\n\u001b[32m    148\u001b[39m random_num = torch.randint(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(val_dataloader.dataset), (\u001b[32m1\u001b[39m,)).item()\n\u001b[32m    149\u001b[39m original_tensor = val_dataloader.dataset[random_num]\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m reconstructed_tensor = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m fig, axes = plt.subplots(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m, figsize=(\u001b[32m16\u001b[39m, \u001b[32m12\u001b[39m))\n\u001b[32m    153\u001b[39m fig.suptitle(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReconstruction at Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, fontsize=\u001b[32m20\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/last_projekt_zespolowy/Lofi-MIDI-VAE-generator/model/model.py:192\u001b[39m, in \u001b[36mLofiModel.reconstruct\u001b[39m\u001b[34m(self, input_pianoroll, output_path)\u001b[39m\n\u001b[32m    190\u001b[39m     reconstructed_pianoroll = torch.argmax(recon_logits, dim=-\u001b[32m1\u001b[39m).squeeze(\u001b[32m0\u001b[39m)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_path:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[43mtensor_to_midi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstructed_pianoroll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m reconstructed_pianoroll\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/last_projekt_zespolowy/Lofi-MIDI-VAE-generator/model/utils.py:178\u001b[39m, in \u001b[36mtensor_to_midi\u001b[39m\u001b[34m(piano_roll_tensor, output_path, ticks_per_beat, tempo_bpm)\u001b[39m\n\u001b[32m    175\u001b[39m     last_event_ticks = current_event_ticks\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Upewnij się, że katalog docelowy istnieje\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m mid.save(output_path)\n\u001b[32m    180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMIDI file saved successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:227\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "train(model, experiment_name=\"model free bits\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lofi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
